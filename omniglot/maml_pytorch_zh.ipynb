{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CSDN-MAML代码踩坑](https://blog.csdn.net/ChaoFeiLi/article/details/108944569) 代码参考 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "root_dir = './data/'\n",
    " \n",
    "img_list = np.load(os.path.join(root_dir, 'omniglot_data.npy'))  # (1623, 20, 1, 28, 28)\n",
    "x_train = img_list[:1200]\n",
    "x_test = img_list[1200:]\n",
    "num_classes = img_list.shape[0]\n",
    "datasets = {'train': x_train, 'test': x_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 准备数据迭代器\n",
    "n_way = 5  ## N-way K-shot在广义上来讲N代表类别数量，K代表每一类别中样本数量\n",
    "# n_way 定义了每个分类任务涉及的类别数。在 N-way K-shot 任务中，这意味着每个任务需要分类的不同类别数为 5。\n",
    "\n",
    "k_spt = 1  ## support data 的个数\n",
    "# k_spt 定义了每个类别在支持集中的样本数量。在这里，每个类别有 1 个样本用于训练模型。\n",
    "\n",
    "k_query = 15  ## query data 的个数\n",
    "# k_query 定义了每个类别在查询集中的样本数量。在这里，每个类别有 15 个样本用于测试模型在学习后的表现。\n",
    "\n",
    "imgsz = 28\n",
    "# imgsz 设置图像的大小，这里图像的尺寸为 28x28 像素。这通常对应于处理的图像数据的分辨率，如 MNIST 手写数字图像。\n",
    "\n",
    "resize = imgsz\n",
    "# resize 通常用于调整图像数据的大小到一个标准的分辨率，这里直接设为 imgsz，意味着不改变原始尺寸。\n",
    "\n",
    "task_num = 8\n",
    "# task_num 定义了每次迭代生成的任务数量。在元学习中，这意味着每次训练迭代中将处理 8 个不同的 N-way K-shot 任务。\n",
    "\n",
    "batch_size = task_num\n",
    "# batch_size 设置为 task_num，意味着每个批次处理的任务数量与 task_num 相等。这在训练元学习模型时，每个批次将包含 8 个任务。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB: train (1200, 20, 1, 28, 28) test (423, 20, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "indexes = {\"train\": 0, \"test\": 0}\n",
    "# `indexes` 字典存储了用于训练集和测试集的当前索引，初始化为0。这些索引可以用来控制从数据集中获取批次数据的位置。后续通过indexes[mode]来获取当前模式的索引。\n",
    "\n",
    "datasets = {\"train\": x_train, \"test\": x_test}\n",
    "# `datasets` 字典将字符串键 \"train\" 和 \"test\" 映射到相应的数据集。x_train 是训练数据集，x_test 是测试数据集。\n",
    "# 这样设置允许代码以统一的方式通过键访问这些数据集，便于在训练和测试过程中加载数据。\n",
    "\n",
    "print(\"DB: train\", x_train.shape, \"test\", x_test.shape)\n",
    "# 这行代码输出训练数据集和测试数据集的形状。数据集的形状通常包括样本数和每个样本的特征数（对于图像数据，可能是三维形状：高度、宽度、颜色通道）。\n",
    "# 打印这些信息有助于验证数据加载正确，且形状符合模型输入的要求。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_cache(dataset):\n",
    "    \"\"\"\n",
    "    Collects several batches data for N-shot learning\n",
    "    :param dataset: [cls_num, 20, 84, 84, 1]\n",
    "    :return: A list with [support_set_x, support_set_y, target_x, target_y] ready to be fed to our networks\n",
    "    \"\"\"\n",
    "    # 定义单个支持集和查询集的大小\n",
    "    setsz = k_spt * n_way  # 每个类支持集样本数 * 类数\n",
    "    querysz = k_query * n_way  # 每个类查询集样本数 * 类数\n",
    "    data_cache = []\n",
    "\n",
    "    # 预加载10个批次的数据，感觉这里的10个批次数据是为了减少数据加载时间\n",
    "    for sample in range(10):  \n",
    "\n",
    "        x_spts, y_spts, x_qrys, y_qrys = [], [], [], []\n",
    "        for i in range(batch_size):  # 每一个批次都包含多个Task\n",
    "\n",
    "            x_spt, y_spt, x_qry, y_qry = [], [], [], []\n",
    "            selected_cls = np.random.choice(dataset.shape[0], n_way, replace=False)  # 随机选择n_way个类\n",
    "\n",
    "            for j, cur_class in enumerate(selected_cls): #生成一个Task 包含n_way个类别 k_spt + k_query\n",
    "                selected_img = np.random.choice(20, k_spt + k_query, replace=False)  # 从每个选中的类中随机选择图片\n",
    "\n",
    "                # 构建支持集和查询集\n",
    "                x_spt.append(dataset[cur_class][selected_img[:k_spt]])  # 支持集图片\n",
    "                x_qry.append(dataset[cur_class][selected_img[k_spt:]])  # 查询集图片\n",
    "                # 从当前类别cur_class中选择前k_spt个图像作为支持集的图像，支持集和查询集的类型是一致的，\n",
    "                # 这里选择前k_spt作为spt，剩下的作为spt。\n",
    "                y_spt.append([j for _ in range(k_spt)])  # 支持集标签\n",
    "                y_qry.append([j for _ in range(k_query)])  # 查询集标签\n",
    "                # 列表推导式(List Comprehension)：[j for _ in range(k_spt)] 生成一个长度为 k_spt 的列表，其中每个元素都是 j。\n",
    "                # 这里 j 是当前类别的索引，用于标记支持集和查询集的标签。\n",
    "\n",
    "            # 批内随机打乱支持集和查询集\n",
    "            perm = np.random.permutation(n_way * k_spt)\n",
    "            # 生成随机索引（perm）：np.random.permutation(n_way * k_spt) 生成一个从 0 到 n_way * k_spt-1 的随机序列，\n",
    "            # 这里 n_way 是类别数，k_spt 是每个类别在支持集中的样本数。这个随机序列用于重新排列支持集中的样本。\n",
    "            x_spt = np.array(x_spt).reshape(n_way * k_spt, 1, resize, resize)[perm]\n",
    "            y_spt = np.array(y_spt).reshape(n_way * k_spt)[perm]\n",
    "            perm = np.random.permutation(n_way * k_query)\n",
    "            x_qry = np.array(x_qry).reshape(n_way * k_query, 1, resize, resize)[perm]\n",
    "            y_qry = np.array(y_qry).reshape(n_way * k_query)[perm]\n",
    "\n",
    "\n",
    "            # 将支持集和查询集添加到对应的列表中\n",
    "            x_spts.append(x_spt)\n",
    "            y_spts.append(y_spt)\n",
    "            x_qrys.append(x_qry)\n",
    "            y_qrys.append(y_qry)\n",
    "\n",
    "        # 将收集的数据转换为适合网络输入的形状\n",
    "        x_spts = np.array(x_spts).astype(np.float32).reshape(batch_size, setsz, 1, resize, resize)\n",
    "        y_spts = np.array(y_spts).astype(np.int64).reshape(batch_size, setsz)\n",
    "        x_qrys = np.array(x_qrys).astype(np.float32).reshape(batch_size, querysz, 1, resize, resize)\n",
    "        y_qrys = np.array(y_qrys).astype(np.int64).reshape(batch_size, querysz)\n",
    "        # 将处理好的一个批次数据添加到数据缓存中,包含了10个批次的数据(并不是一个epoch的概念)，这一步只是为了减少数据加载时间，不是必须的\n",
    "        data_cache.append([x_spts, y_spts, x_qrys, y_qrys])\n",
    "        \n",
    "    return data_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 创建一个字典，存储训练集和测试集的数据缓存\n",
    "datasets_cache = {\"train\": load_data_cache(x_train),  # 加载并缓存训练数据\n",
    "                  \"test\": load_data_cache(x_test)}     # 加载并缓存测试数据\n",
    "\n",
    "def next(mode='train'):\n",
    "    \"\"\"\n",
    "    从数据集中获取下一个批次的数据。\n",
    "    :param mode: 数据集的分割名称（\"train\"、\"val\" 或 \"test\" 其中之一）\n",
    "    :return: 返回下一个数据批次\n",
    "    \"\"\"\n",
    "    # 如果当前索引大于或等于数据缓存的长度，重置索引并重新加载数据到缓存\n",
    "    if indexes[mode] >= len(datasets_cache[mode]):\n",
    "        indexes[mode] = 0  # 重置索引\n",
    "        datasets_cache[mode] = load_data_cache(datasets[mode])  # 重新加载数据到缓存\n",
    "\n",
    "    # 从缓存中获取下一批数据\n",
    "    next_batch = datasets_cache[mode][indexes[mode]]\n",
    "    indexes[mode] += 1  # 更新索引以指向下一个批次\n",
    "    x_spts, y_spts, x_qrys, y_qrys = next_batch\n",
    "    return next_batch  # 返回获取的批次数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from copy import deepcopy, copy\n",
    " \n",
    " \n",
    "class BaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseNet, self).__init__()\n",
    "        self.vars = nn.ParameterList()  # 存储所有可训练参数的列表\n",
    "        self.vars_bn = nn.ParameterList()  # 存储批处理归一化层的运行时参数\n",
    "\n",
    " \n",
    "        # 第1个conv2d\n",
    "        # in_channels = 1, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2\n",
    "        weight = nn.Parameter(torch.ones(64, 1, 3, 3))\n",
    "        nn.init.kaiming_normal_(weight)\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    " \n",
    "        # 第1个BatchNorm层\n",
    "        weight = nn.Parameter(torch.ones(64))\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    " \n",
    "        running_mean = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        running_var = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        self.vars_bn.extend([running_mean, running_var])\n",
    " \n",
    "        # 第2个conv2d\n",
    "        # in_channels = 1, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2\n",
    "        weight = nn.Parameter(torch.ones(64, 64, 3, 3))\n",
    "        nn.init.kaiming_normal_(weight)\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    " \n",
    "        # 第2个BatchNorm层\n",
    "        weight = nn.Parameter(torch.ones(64))\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    " \n",
    "        running_mean = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        running_var = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        self.vars_bn.extend([running_mean, running_var])\n",
    " \n",
    "        # 第3个conv2d\n",
    "        # in_channels = 1, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2\n",
    "        weight = nn.Parameter(torch.ones(64, 64, 3, 3))\n",
    "        nn.init.kaiming_normal_(weight)\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    " \n",
    "        # 第3个BatchNorm层\n",
    "        weight = nn.Parameter(torch.ones(64))\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    " \n",
    "        running_mean = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        running_var = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        self.vars_bn.extend([running_mean, running_var])\n",
    " \n",
    "        # 第4个conv2d\n",
    "        # in_channels = 1, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2\n",
    "        weight = nn.Parameter(torch.ones(64, 64, 3, 3))\n",
    "        nn.init.kaiming_normal_(weight)\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    " \n",
    "        # 第4个BatchNorm层\n",
    "        weight = nn.Parameter(torch.ones(64))\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    " \n",
    "        running_mean = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        running_var = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        self.vars_bn.extend([running_mean, running_var])\n",
    " \n",
    "        ##linear\n",
    "        weight = nn.Parameter(torch.ones([5, 64]))\n",
    "        bias = nn.Parameter(torch.zeros(5))\n",
    "        self.vars.extend([weight, bias])\n",
    " \n",
    "    def forward(self, x, params=None, bn_training=True):\n",
    "        '''\n",
    "        定义模型的前向传播\n",
    "        :param x: 输入数据\n",
    "        :param params: 外部传入的参数列表，用于一些特定情境\n",
    "        :param bn_training: 是否在训练模式下运行批处理归一化\n",
    "        :return: 模型的输出\n",
    "        '''\n",
    "        if params is None:\n",
    "            params = self.vars\n",
    " \n",
    "        weight, bias = params[0], params[1]  # 第1个CONV层\n",
    "        x = F.conv2d(x, weight, bias, stride=2, padding=2)\n",
    " \n",
    "        weight, bias = params[2], params[3]  # 第1个BN层\n",
    "        running_mean, running_var = self.vars_bn[0], self.vars_bn[1]\n",
    "        x = F.batch_norm(x, running_mean, running_var, weight=weight, bias=bias, training=bn_training)\n",
    "        x = F.max_pool2d(x, kernel_size=2)  # 第1个MAX_POOL层\n",
    "        x = F.relu(x, inplace=[True])  # 第1个relu\n",
    " \n",
    "        weight, bias = params[4], params[5]  # 第2个CONV层\n",
    "        x = F.conv2d(x, weight, bias, stride=2, padding=2)\n",
    " \n",
    "        weight, bias = params[6], params[7]  # 第2个BN层\n",
    "        running_mean, running_var = self.vars_bn[2], self.vars_bn[3]\n",
    "        x = F.batch_norm(x, running_mean, running_var, weight=weight, bias=bias, training=bn_training)\n",
    "        x = F.max_pool2d(x, kernel_size=2)  # 第2个MAX_POOL层\n",
    "        x = F.relu(x, inplace=[True])  # 第2个relu\n",
    " \n",
    "        weight, bias = params[8], params[9]  # 第3个CONV层\n",
    "        x = F.conv2d(x, weight, bias, stride=2, padding=2)\n",
    " \n",
    "        weight, bias = params[10], params[11]  # 第3个BN层\n",
    "        running_mean, running_var = self.vars_bn[4], self.vars_bn[5]\n",
    "        x = F.batch_norm(x, running_mean, running_var, weight=weight, bias=bias, training=bn_training)\n",
    "        x = F.max_pool2d(x, kernel_size=2)  # 第3个MAX_POOL层\n",
    "        x = F.relu(x, inplace=[True])  # 第3个relu\n",
    " \n",
    "        weight, bias = params[12], params[13]  # 第4个CONV层\n",
    "        x = F.conv2d(x, weight, bias, stride=2, padding=2)\n",
    "        x = F.relu(x, inplace=[True])  # 第4个relu\n",
    "        weight, bias = params[14], params[15]  # 第4个BN层\n",
    "        running_mean, running_var = self.vars_bn[6], self.vars_bn[7]\n",
    "        x = F.batch_norm(x, running_mean, running_var, weight=weight, bias=bias, training=bn_training)\n",
    "        x = F.max_pool2d(x, kernel_size=2)  # 第4个MAX_POOL层\n",
    " \n",
    "        x = x.view(x.size(0), -1)  ## flatten\n",
    "        weight, bias = params[16], params[17]  # linear\n",
    "        x = F.linear(x, weight, bias)\n",
    " \n",
    "        output = x\n",
    " \n",
    "        return output\n",
    " \n",
    "    def parameters(self):\n",
    "        return self.vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from copy import deepcopy\n",
    "\n",
    "class MetaLearner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        self.update_step = 5  # 任务级别的内部更新步骤数\n",
    "        self.update_step_test = 5  # 测试时的更新步骤数\n",
    "        self.net = BaseNet()  # 使用自定义的基础网络\n",
    "        self.meta_lr = 2e-4  # 元学习率\n",
    "        self.base_lr = 4 * 1e-2  # 基本学习率\n",
    "        self.inner_lr = 0.4  # 内部循环的学习率\n",
    "        self.outer_lr = 1e-2  # 外部循环的学习率\n",
    "        self.meta_optim = torch.optim.Adam(self.net.parameters(), lr=self.meta_lr)  # 元优化器\n",
    "\n",
    "    def forward(self, x_spt, y_spt, x_qry, y_qry):\n",
    "        # 初始化\n",
    "        task_num, ways, shots, h, w = x_spt.size()  # 解析支持集的维度\n",
    "        query_size = x_qry.size(1)  # 查询集的大小\n",
    "        loss_list_qry = [0 for _ in range(self.update_step + 1)]\n",
    "        correct_list = [0 for _ in range(self.update_step + 1)]\n",
    "\n",
    "        for i in range(task_num):  # 遍历每个任务\n",
    "            y_hat = self.net(x_spt[i], params=None, bn_training=True)  # 第0步更新\n",
    "            loss = F.cross_entropy(y_hat, y_spt[i])  # 计算交叉熵损失\n",
    "            grad = torch.autograd.grad(loss, self.net.parameters())  # 计算梯度\n",
    "            tuples = zip(grad, self.net.parameters())  # 梯度和参数配对\n",
    "            fast_weights = list(map(lambda p: p[1] - self.base_lr * p[0], tuples))  # 应用梯度更新\n",
    "\n",
    "            # 在query集上计算损失和准确率\n",
    "            with torch.no_grad():\n",
    "                y_hat = self.net(x_qry[i], self.net.parameters(), bn_training=True)\n",
    "                loss_qry = F.cross_entropy(y_hat, y_qry[i])\n",
    "                loss_list_qry[0] += loss_qry\n",
    "                pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)\n",
    "                correct = torch.eq(pred_qry, y_qry[i]).sum().item()\n",
    "                correct_list[0] += correct\n",
    "\n",
    "            for k in range(1, self.update_step):  # 进行更多的更新步骤\n",
    "                y_hat = self.net(x_spt[i], params=fast_weights, bn_training=True)\n",
    "                loss = F.cross_entropy(y_hat, y_spt[i])\n",
    "                grad = torch.autograd.grad(loss, fast_weights)\n",
    "                tuples = zip(grad, fast_weights)\n",
    "                fast_weights = list(map(lambda p: p[1] - self.base_lr * p[0], tuples))\n",
    "\n",
    "                y_hat = self.net(x_qry[i], params=fast_weights, bn_training=True)\n",
    "                loss_qry = F.cross_entropy(y_hat, y_qry[i])\n",
    "                loss_list_qry[k + 1] += loss_qry\n",
    "                with torch.no_grad():\n",
    "                    pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)\n",
    "                    correct = torch.eq(pred_qry, y_qry[i]).sum().item()\n",
    "                    correct_list[k + 1] += correct\n",
    "\n",
    "        # 计算整体损失和准确率，然后进行梯度下降\n",
    "        loss_qry = loss_list_qry[-1] / task_num\n",
    "        self.meta_optim.zero_grad()\n",
    "        loss_qry.backward()\n",
    "        self.meta_optim.step()\n",
    "\n",
    "        accs = np.array(correct_list) / (query_size * task_num)  # 计算平均准确率\n",
    "        loss = np.array(loss_list_qry) / (task_num)  # 计算平均损失\n",
    "        return accs, loss\n",
    "\n",
    "    def finetunning(self, x_spt, y_spt, x_qry, y_qry):\n",
    "        assert len(x_spt.shape) == 4\n",
    "        \n",
    "        query_size = x_qry.size(0)\n",
    "        correct_list = [0 for _ in range(self.update_step_test + 1)]\n",
    "\n",
    "        new_net = deepcopy(self.net)  # 深拷贝网络进行微调\n",
    "        y_hat = new_net(x_spt)\n",
    "        loss = F.cross_entropy(y_hat, y_spt)\n",
    "        grad = torch.autograd.grad(loss, new_net.parameters())\n",
    "        fast_weights = list(map(lambda p: p[1] - self.base_lr * p[0], zip(grad, new_net.parameters())))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_hat = new_net(x_qry, params=new_net.parameters(), bn_training=True)\n",
    "            pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)\n",
    "            correct = torch.eq(pred_qry, y_qry).sum().item()\n",
    "            correct_list[0] += correct\n",
    "\n",
    "        for k in range(1, self.update_step_test):\n",
    "            y_hat = new_net(x_spt, params=fast_weights, bn_training=True)\n",
    "            loss = F.cross_entropy(y_hat, y_spt)\n",
    "            grad = torch.autograd.grad(loss, fast_weights)\n",
    "            fast_weights = list(map(lambda p: p[1] - self.base_lr * p[0], zip(grad, fast_weights)))\n",
    "\n",
    "            y_hat = new_net(x_qry, fast_weights, bn_training=True)\n",
    "            with torch.no_grad():\n",
    "                pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)\n",
    "                correct = torch.eq(pred_qry, y_qry).sum().item()\n",
    "                correct_list[k + 1] += correct\n",
    "\n",
    "        del new_net\n",
    "        accs = np.array(correct_list) / query_size\n",
    "        return accs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0-----------------------------------\n",
      "Epoch: 0, Time: 2.81s\n",
      "Training Accuracies: [0.2        0.         0.325      0.34166667 0.33666667 0.37333333]\n",
      "Training Loss: [1.60943782 0.         1.53812814 1.51770508 1.5066402  1.46387494]\n",
      "Accuracies array shape before mean: (96, 6)\n",
      "Test Accuracies: [0.2    0.     0.3406 0.3643 0.391  0.403 ]\n",
      "Epoch: 10-----------------------------------\n",
      "Epoch: 10, Time: 2.39s\n",
      "Training Accuracies: [0.23       0.         0.42       0.455      0.47666667 0.45833333]\n",
      "Training Loss: [1.60988963 0.         1.49685156 1.45297253 1.40584671 1.36951578]\n",
      "Epoch: 20-----------------------------------\n",
      "Epoch: 20, Time: 2.31s\n",
      "Training Accuracies: [0.2        0.         0.515      0.54166667 0.55166667 0.55166667]\n",
      "Training Loss: [1.61023092 0.         1.38320374 1.30579901 1.24969971 1.2088201 ]\n",
      "Epoch: 30-----------------------------------\n",
      "Epoch: 30, Time: 2.67s\n",
      "Training Accuracies: [0.2        0.         0.53333333 0.56166667 0.56       0.56333333]\n",
      "Training Loss: [1.61258554 0.         1.34505808 1.26245666 1.21582103 1.17570615]\n",
      "Epoch: 40-----------------------------------\n",
      "Epoch: 40, Time: 2.51s\n",
      "Training Accuracies: [0.2        0.         0.50833333 0.555      0.55166667 0.555     ]\n",
      "Training Loss: [1.61295569 0.         1.29616165 1.21853101 1.18155634 1.15242994]\n",
      "Epoch: 50-----------------------------------\n",
      "Epoch: 50, Time: 2.42s\n",
      "Training Accuracies: [0.19833333 0.         0.61166667 0.64666667 0.65333333 0.655     ]\n",
      "Training Loss: [1.61311841 0.         1.17740345 1.08008409 1.02412701 0.99297404]\n",
      "Epoch: 60-----------------------------------\n",
      "Epoch: 60, Time: 2.48s\n",
      "Training Accuracies: [0.2        0.         0.595      0.61       0.63166667 0.62166667]\n",
      "Training Loss: [1.61619747 0.         1.20265853 1.11709249 1.06661296 1.03958035]\n",
      "Epoch: 70-----------------------------------\n",
      "Epoch: 70, Time: 2.42s\n",
      "Training Accuracies: [0.20833333 0.         0.66333333 0.67166667 0.67833333 0.68      ]\n",
      "Training Loss: [1.61138248 0.         1.11027813 1.01749361 0.96202588 0.93342745]\n",
      "Epoch: 80-----------------------------------\n",
      "Epoch: 80, Time: 2.39s\n",
      "Training Accuracies: [0.205      0.         0.665      0.685      0.67833333 0.66833333]\n",
      "Training Loss: [1.61406946 0.         1.11111724 1.02459049 0.97860926 0.95577413]\n",
      "Epoch: 90-----------------------------------\n",
      "Epoch: 90, Time: 2.43s\n",
      "Training Accuracies: [0.175      0.         0.55333333 0.54666667 0.54833333 0.55166667]\n",
      "Training Loss: [1.61060357 0.         1.23749566 1.17854226 1.14995682 1.13922679]\n",
      "Epoch: 100-----------------------------------\n",
      "Epoch: 100, Time: 2.40s\n",
      "Training Accuracies: [0.17666667 0.         0.68333333 0.70333333 0.71333333 0.72      ]\n",
      "Training Loss: [1.60764313 0.         1.0608958  0.96170616 0.90772164 0.87736452]\n",
      "Accuracies array shape before mean: (96, 6)\n",
      "Test Accuracies: [0.1895 0.     0.5767 0.589  0.5933 0.5957]\n",
      "Epoch: 110-----------------------------------\n",
      "Epoch: 110, Time: 2.28s\n",
      "Training Accuracies: [0.155      0.         0.62833333 0.65333333 0.66       0.66666667]\n",
      "Training Loss: [1.61183846 0.         1.09099686 0.99992669 0.95090383 0.92169094]\n",
      "Epoch: 120-----------------------------------\n",
      "Epoch: 120, Time: 2.39s\n",
      "Training Accuracies: [0.20333333 0.         0.58       0.585      0.6        0.59833333]\n",
      "Training Loss: [1.60731864 0.         1.14290452 1.07339585 1.03362572 1.01608634]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_861/3003377916.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# 打印形状\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0maccs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_spt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_spt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_qry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_qry\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 执行前向传播并获取准确率和损失\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 记录结束时间\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_861/2555479899.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_spt, y_spt, x_qry, y_qry)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mfast_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_lr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_qry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfast_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 \u001b[0mloss_qry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_qry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mloss_list_qry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_qry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_861/2470717932.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, params, bn_training)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars_bn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars_bn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbn_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 第1个MAX_POOL层\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 第1个relu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# 实例化MetaLearner模型并移至设备\n",
    "meta = MetaLearner().to(device)\n",
    "epochs = 60000  # 设置迭代次数\n",
    "\n",
    "for step in range(epochs):\n",
    "    start = time.time()  # 记录开始时间\n",
    "    x_spt, y_spt, x_qry, y_qry = next('train')  # 获取训练数据\n",
    "    # 将数据转换为torch tensor并移动到指定设备\n",
    "    x_spt, y_spt, x_qry, y_qry = torch.from_numpy(x_spt).to(device), torch.from_numpy(y_spt).long().to(\n",
    "        device), torch.from_numpy(x_qry).to(device), torch.from_numpy(y_qry).long().to(device)\n",
    "    # 打印形状\n",
    "\n",
    "    accs, loss = meta(x_spt, y_spt, x_qry, y_qry)  # 执行前向传播并获取准确率和损失\n",
    "    end = time.time()  # 记录结束时间\n",
    "    \n",
    "    if step % 10 == 0:  # 每10步打印一次结果\n",
    "        print(f\"Epoch: {step}-----------------------------------\")\n",
    "        print(f\"Epoch: {step}, Time: {end - start:.2f}s\")\n",
    "        print(f\"Training Accuracies: {accs}\")\n",
    "        print(f\"Training Loss: {loss}\")\n",
    "\n",
    "    if step % 100 == 0:  # 每100步进行一次更细致的测试\n",
    "        accs = []\n",
    "        for _ in range(100 // task_num):  # 按任务数量进行测试迭代\n",
    "            x_spt, y_spt, x_qry, y_qry = next('test')  # 获取测试数据\n",
    "            x_spt, y_spt, x_qry, y_qry = torch.from_numpy(x_spt).to(device), torch.from_numpy(y_spt).long().to(\n",
    "                device), torch.from_numpy(x_qry).to(device), torch.from_numpy(y_qry).long().to(device)\n",
    "            for x_spt_one, y_spt_one, x_qry_one, y_qry_one in zip(x_spt, y_spt, x_qry, y_qry):  # 对每一个任务的样本进行微调测试\n",
    "                test_acc = meta.finetunning(x_spt_one, y_spt_one, x_qry_one, y_qry_one)\n",
    "                accs.append(test_acc)\n",
    "        accs = np.array(accs).mean(axis=0).astype(np.float16)  # 计算所有测试准确率的平均值\n",
    "        print(f'Test Accuracies: {accs}')  # 打印平均测试准确率\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
